{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2089fd55974f42c19e2098b06a94ec05",
    "deepnote_cell_height": 82,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Aave Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical liquidations, size of position and associated tokens\n",
    "query_TotalLiquidations = \"\"\"\n",
    "query TotalLiquidations($txnID: ID) {\n",
    "  liquidates(first: 1000, where: {id_gt: $txnID}) {\n",
    "    hash\n",
    "    amountUSD\n",
    "    timestamp\n",
    "    liquidatee {\n",
    "      id\n",
    "    }\n",
    "    asset {\n",
    "      name\n",
    "      symbol\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical deposits, size of position and associated tokens\n",
    "query_CollateralSuppliedDeposits = \"\"\"\n",
    "query CollateralSuppliedDeposits($txnID: ID) {\n",
    "  deposits (first: 1000, where: {id_gt: $txnID}) {\n",
    "    hash\n",
    "    amountUSD\n",
    "    timestamp\n",
    "    account {\n",
    "      id\n",
    "    }\n",
    "    asset {\n",
    "      name\n",
    "      symbol\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical withdraws, size of position and associated tokens\n",
    "query_CollateralSuppliedWithdraws = \"\"\"\n",
    "query CollateralSuppliedWithdraws($txnID: ID) {\n",
    "  withdraws(first: 1000, where: {id_gt: $txnID}) {\n",
    "    hash\n",
    "    amountUSD\n",
    "    timestamp\n",
    "    account {\n",
    "      id\n",
    "    }\n",
    "    asset {\n",
    "      name\n",
    "      symbol\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical borrows, size of position and associated tokens\n",
    "query_Borrows = \"\"\"\n",
    "query Borrows($txnID: ID) {\n",
    "  borrows(first: 1000, where: {id_gt: $txnID}) {\n",
    "    hash\n",
    "    amountUSD\n",
    "    timestamp\n",
    "    account {\n",
    "      id\n",
    "    }\n",
    "    asset {\n",
    "      name\n",
    "      symbol\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical repays, size of position and associated tokens\n",
    "query_Repays = \"\"\"\n",
    "query Repays($txnID: ID) {\n",
    "  repays(first: 1000, where: {id_gt: $txnID}) {\n",
    "    hash\n",
    "    amountUSD\n",
    "    timestamp\n",
    "    account {\n",
    "      id\n",
    "    }\n",
    "    asset {\n",
    "      name\n",
    "      symbol\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical borrows and their associated rate type: variable or stable\n",
    "query_ratetype = \"\"\"\n",
    "query ratetype($txnID: ID) {\n",
    "  borrows(first: 1000, where: {id_gt: $txnID}) {\n",
    "    id\n",
    "    borrowRateMode\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "24507dbd3c1d4b2eb6a4e089dafe5ac0",
    "deepnote_cell_height": 70,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "ea3661ced622423f935217e37d6dba64",
    "deepnote_cell_height": 130,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 9050582,
    "execution_start": 1659481728871,
    "source_hash": "4daec8c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime,timedelta\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "29ec8e40b9bf425294bfdbd159699de0",
    "deepnote_cell_height": 70,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Data Set Up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "2aba4f431e8043758920685865bba98e",
    "deepnote_cell_height": 148,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 9050537,
    "execution_start": 1659481728870,
    "source_hash": "683d0683",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General Format --> {[query] : ([parent_entity], [child_entity], [type_of_data])}\n",
    "\n",
    "query_pairs = {query_CollateralSuppliedDeposits    : (\"deposits\",\"hash\", \"txn_data\"),\n",
    "               query_CollateralSuppliedWithdraws   : (\"withdraws\",\"hash\", \"txn_data\"),\n",
    "               query_TotalLiquidations             : (\"liquidates\",\"hash\", \"txn_data\"),\n",
    "               query_Borrows                       : (\"borrows\",\"hash\", \"txn_data\"),\n",
    "               query_Repays                        : (\"repays\",\"hash\", \"txn_data\"),\n",
    "               query_ratetype                      : (\"borrows\", \"id\", \"param_data\")}\n",
    "\n",
    "# General Format --> {([Version], [Market], [type_of_data]) : [Subgraph_Api_Endpoint]}\n",
    "\n",
    "api_dict = {(\"V2\", \"Ethereum\", \"txn_data\")    : \"https://api.thegraph.com/subgraphs/name/messari/aave-v2-ethereum\",\n",
    "            (\"V2\", \"Avalanche\", \"txn_data\")   : \"https://api.thegraph.com/subgraphs/name/messari/aave-v2-avalanche\",\n",
    "            (\"V2\", \"Ethereum\", \"param_data\")  : \"https://api.thegraph.com/subgraphs/name/aave/protocol-v2\",\n",
    "            (\"V2\", \"Avalanche\", \"param_data\") : \"https://api.thegraph.com/subgraphs/name/aave/protocol-v2-avalanche\"}\n",
    "\n",
    "# Sidelined for now\n",
    "# (\"V2\", \"Polygon\", \"txn_data\")     : \"https://api.thegraph.com/subgraphs/name/messari/aave-v2-polygon\"\n",
    "# (\"V2\", \"Polygon\", \"param_data\")   : \"https://api.thegraph.com/subgraphs/name/aave/aave-v2-matic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframes to Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dict = {}                                                               \n",
    "\n",
    "for query,attributes in query_pairs.items():\n",
    "    for (version,market,data) in api_dict.keys():\n",
    "        if attributes[2] == data:\n",
    "            query_dict[(version,market,attributes[2],attributes[0],attributes[1],query)] = pd.DataFrame()\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagination Collection of all Entries from Subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "cell_id": "6fcfe3ad243b485594ca1e1d9667ac01",
    "deepnote_cell_height": 890.78125,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 330885,
    "execution_start": 1659481728872,
    "source_hash": "de9292fc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "entity = 0\n",
    "count = 1\n",
    "\n",
    "# Iterate through all queries\n",
    "for attributes in query_dict.keys():\n",
    "    \n",
    "    # Assign last transaction hash\n",
    "    txnID = {'txnID' : \"\"}                                                    \n",
    "\n",
    "    while True:\n",
    "        \n",
    "        version, market, query = attributes[0],attributes[1], attributes[5]\n",
    "        \n",
    "        # Must update \n",
    "        api_endpoint = api_dict[(version,market,attributes[2])]\n",
    "        request = requests.post(api_endpoint, json={'query': query,'variables': txnID})  \n",
    "        \n",
    "        # MUST UPDATE INDEXING LOGIC BASED ON QUERY\n",
    "        df_test = request.json()\n",
    "        df_flatten = pd.json_normalize((df_test[\"data\"][attributes[3]]))       \n",
    "                \n",
    "        # MUST CREATE NEW DATAFRAME BASED ON QUERY\n",
    "        query_dict[attributes] = query_dict[attributes].append(df_flatten, ignore_index = True) \n",
    "        txnID_upd = {'txnID' : df_flatten[attributes[4]][len(df_flatten)-1]}\n",
    "        print(query_dict[attributes].shape)\n",
    "        \n",
    "        txnID.update(txnID_upd)\n",
    "\n",
    "        if len(df_flatten) == 1000:\n",
    "            count += 1\n",
    "            continue\n",
    "        else:\n",
    "            count += 1\n",
    "            break\n",
    "    entity = entity + 1\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *TEMP DOCUMENT IMPORT/EXPORT*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export files to store for temporary use --> update file path \n",
    "for keys,df in query_dict.items():\n",
    "    filename = 'PATH_NAME'+str(keys[:5])\n",
    "    df.to_parquet(filename+\".parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Read files --> update file path \n",
    "for keys,df in query_dict.items():\n",
    "    filename = 'PATH_NAME'+str(keys[:5])+'.parquet'\n",
    "    try:\n",
    "        query_dict[keys] = pd.read_parquet(filename)\n",
    "        print(\"Success\")\n",
    "    except:\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Off Manipulation Step for Liquidations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update column \"liquidatee\" to \"account.id\"\n",
    "\n",
    "# Iterate through all query dataframes\n",
    "for keys,df in query_dict.items():\n",
    "    for col in df.columns:\n",
    "        if col == \"liquidatee.id\":\n",
    "            df.rename(columns={\"liquidatee.id\": \"account.id\"}, inplace=True)\n",
    "            \n",
    "        elif col == \"reserve.name\" in df.columns:\n",
    "            df.rename(columns={\"reserve.name\": \"asset.name\"}, inplace=True)\n",
    "            \n",
    "        elif col == \"reserve.symbol\" in df.columns:\n",
    "            df.rename(columns={\"reserve.symbol\": \"asset.symbol\"}, inplace=True)\n",
    "        \n",
    "        # Ethereum and Avalanche store txn hash differently compared to Polygon\n",
    "        elif col == 'id' in df.columns:\n",
    "            df[\"id\"] = df[\"id\"].str.split(\":\", expand = True)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e81ceeb563c24fb6bb0a3107d000277b",
    "deepnote_cell_height": 70,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Unique Identifier Manipulation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "f6e6ccf9da8e4357990dddcac6a302e0",
    "deepnote_cell_height": 333,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 40726,
    "execution_start": 1659482059435,
    "source_hash": "63049d9a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 37s, sys: 1.74 s, total: 2min 38s\n",
      "Wall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Iterate through all query dataframes\n",
    "for keys,df in query_dict.items():\n",
    "\n",
    "    # Add column \"hash_unique\"\n",
    "    df[keys[4]+\"_unique\"] = \"\"\n",
    "    \n",
    "    # Create dictionary of hashes with 0 as the initial counter value\n",
    "    current_counter = {}\n",
    "    for ID in df[keys[4]].value_counts().index:\n",
    "        current_counter[ID] = 0\n",
    "\n",
    "    # Append add'l identifier to \"hash\" in \"hash_unique\"\n",
    "    for i,r in df.iterrows():\n",
    "        r[keys[4]+\"_unique\"] = r[keys[4]] +\"_\"+str(len(keys[3]))+str(current_counter[r[keys[4]]])\n",
    "        current_counter[r[keys[4]]]+=1\n",
    "\n",
    "    # Drop child_entity column\n",
    "     # df.drop(keys[4], axis=1, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "94b09fe05c864ceb90c5c8ba95a3d96c",
    "deepnote_cell_height": 70,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Conversion Steps of Dataframes: Timestamp and amountUSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_id": "8602ebafa0874b75a2b08acf4e7d6fc3",
    "deepnote_cell_height": 130,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 35663,
    "execution_start": 1659482100193,
    "source_hash": "4e095eb8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('V2', 'Ethereum', 'param_data', 'borrows', 'id') asset.symbol not in dataframe.\n",
      "('V2', 'Ethereum', 'param_data', 'borrows', 'id') timestamp not in dataframe.\n",
      "('V2', 'Ethereum', 'param_data', 'borrows', 'id') amountUSD not in dataframe.\n",
      "('V2', 'Avalanche', 'param_data', 'borrows', 'id') asset.symbol not in dataframe.\n",
      "('V2', 'Avalanche', 'param_data', 'borrows', 'id') timestamp not in dataframe.\n",
      "('V2', 'Avalanche', 'param_data', 'borrows', 'id') amountUSD not in dataframe.\n",
      "CPU times: user 4.32 s, sys: 659 ms, total: 4.97 s\n",
      "Wall time: 5.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Iterate through all query dataframes\n",
    "for keys,df in query_dict.items():\n",
    "\n",
    "    # Remove all '.e' extensions under symbol\n",
    "    try:\n",
    "        df[\"asset.symbol\"] = df[\"asset.symbol\"].str.upper()\n",
    "        df[\"asset.symbol\"] = df['asset.symbol'].str.replace(\".E\", \"\", regex=False)\n",
    "    except:\n",
    "        print(keys[:5], \"asset.symbol not in dataframe.\")\n",
    "    \n",
    "    # Convert all timestamps to datetime format\n",
    "    try:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "    except:\n",
    "        print(keys[:5], \"timestamp not in dataframe.\")\n",
    "    \n",
    "    # Convert 'amountUSD' to type float\n",
    "    try:\n",
    "        df['amountUSD'] = df['amountUSD'].astype(float)\n",
    "    except:\n",
    "        print(keys[:5], \"amountUSD not in dataframe.\")\n",
    "    \n",
    "    # Update transaction id to transaction hash\n",
    "    try:\n",
    "        df.rename(columns={\"id_unique\": \"hash_unique\"}, inplace=True)\n",
    "    except:\n",
    "        print(keys[:5], \"id_unique not in dataframe.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Version, Market, and Event Type to Each Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.3 ms, sys: 15.6 ms, total: 56.9 ms\n",
      "Wall time: 80.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for keys,df in query_dict.items():\n",
    "    \n",
    "    # Append version type\n",
    "    df['version'] = keys[0]\n",
    "    \n",
    "    # Append market type\n",
    "    df['market'] = keys[1]\n",
    "    \n",
    "    # Append event type\n",
    "    df['event_type'] = keys[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import / Create Grades of Tokens (Aave Risk Team Provided) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many UST transactions happened and the amount\n",
    "# Check RENFIL and UST transactions since they are not allowed to be used as collateral\n",
    "# ADDED: STETH to 2021/09/24 based on risk parameters https://docs.aave.com/risk/v/aave-v2/asset-risk/risk-parameters\n",
    "# ADDED: ENS TO 2021/09/24 based on risk parameters https://docs.aave.com/risk/v/aave-v2/asset-risk/risk-parameters\n",
    "# ADDED: WAVAX to 2021/09/24 based on risk parameters https://docs.aave.com/risk/v/aave-v2/asset-risk/risk-parameters\n",
    "# ADDED BAL TO 2021/01/14\n",
    "\n",
    "### NEED TO UPDATE RISK OF CVX\n",
    "\n",
    "token_risk_dict = {(\"Ethereum\", \"2021/09/24\") :\n",
    "                  {'AMPL'  : 'B',  'BUSD'  : 'B+', 'DAI'  : 'B+', 'FEI'  : 'B-', 'FRAX' : 'B-', \n",
    "                   'GUSD'  : 'B-', 'PAX'   : 'B+', 'RAI'  : 'C+', 'SUSD' : 'B-', 'TUSD' : 'B-',\n",
    "                   'USDC'  : 'A-', 'USDT'  : 'B+', 'AAVE' : 'B',  'BAL'  : 'B',  'BAT'  : 'B+', 'CRV'  : 'B',\n",
    "                   'DPI'   : 'C+', 'ENJ'   : 'B+', 'ETH'  : 'A+', 'KNC'  : 'B',  'LINK' : 'A-', 'MANA' : 'B',\n",
    "                   'MKR'   : 'B',  'REN'   : 'B',  'SNX'  : 'B',  'UNI'  : 'B+', 'WBTC' : 'B',  'WETH' : 'A',\n",
    "                   'WMATIC': 'B+', 'XSUSHI': 'B-', 'YFI'  : 'B+', 'ZRX'  : 'B+', 'STETH': 'B+', 'ENS'  : 'B',\n",
    "                   'WAVAX' : 'B',  'RENFIL': '0',  'UST'  : '0',  '1INCH': '0',  'CVX'  : 'B'},\n",
    "                   \n",
    "                   (\"Ethereum\", \"2021/08/10\") :\n",
    "                  {'AMPL'  : 'B-', 'BUSD'  : 'B+', 'DAI'  : 'B+', 'GUSD' : 'B-', 'PAX'  : 'B+', 'RAI'  : 'C+',\n",
    "                   'SUSD'  : 'B-', 'TUSD'  : 'B-', 'USDC' : 'A-', 'USDT' : 'B+', 'AAVE' : 'B',  'BAL'  : 'B',\n",
    "                   'BAT'   : 'B+', 'CRV'   : 'B',  'ENJ'  : 'B+', 'KNC'  : 'B',  'LINK' : 'A-', 'MANA' : 'B',\n",
    "                   'MKR'   : 'B-', 'REN'   : 'B',  'SNX'  : 'B',  'UNI'  : 'B',  'WBTC' : 'B',  'WETH' : 'A-', \n",
    "                   'XSUSHI': 'B-', 'YFI'   : 'B',  'ZRX'  : 'B+', 'RENFIL':'0',  'UST'  : '0',  '1INCH': '0',\n",
    "                   'DPI'   : 'C+'},\n",
    "                   \n",
    "                   (\"Ethereum\", \"2021/02/24\") : \n",
    "                  {'BUSD'  : 'B',  'DAI'   : 'B+', 'GUSD' : 'B-', 'SUSD' : 'B-', 'TUSD' : 'B',  'USDC' : 'A-', \n",
    "                   'USDT'  : 'B+', 'AAVE'  : 'B',  'BAL'  : 'B',  'BAT'  : 'B+', 'CRV'  : 'B',  'ENJ'  : 'B+', \n",
    "                   'KNC'   : 'B+', 'LINK'  : 'B+', 'MANA' : 'B-', 'MKR'  : 'B-', 'REN'  : 'B',  'SNX'  : 'C+', \n",
    "                   'UNI'   : 'B',  'WBTC'  : 'B',  'WETH' : 'A-', 'XSUSHI':'B-', 'YFI'  : 'B',  'ZRX'  : 'B+',\n",
    "                   'RENFIL': '0',  'UST'   : '0',  '1INCH': '0'},\n",
    "                   \n",
    "                   (\"Ethereum\", \"2021/01/14\") :\n",
    "                  {'BUSD'  : 'B',  'DAI'   : 'B+', 'GUSD' : 'B-', 'SUSD' : 'B-', 'TUSD' : 'B',  'USDC': 'A-', \n",
    "                   'USDT'  : 'B+', 'AAVE'  : 'B-', 'BAT'  : 'B+', 'CRV'  : 'B-', 'ENJ'  : 'B',  'KNC' : 'B', \n",
    "                   'LINK'  : 'B+', 'MANA'  : 'B-', 'MKR'  : 'B-', 'REN'  : 'B',  'SNX'  : 'C+', 'UNI' : 'B', \n",
    "                   'WBTC'  : 'B-', 'WETH'  : 'A-', 'YFI'  : 'B',  'ZRX'  : 'B+', 'RENFIL':'0',  'UST' : '0',\n",
    "                   'BAL'   : 'B'},\n",
    "                   \n",
    "                   (\"Ethereum\", \"2020/12/01\") :\n",
    "                  {'BUSD'  : 'B',  'DAI'   : 'B+', 'SUSD' : 'C+', 'TUSD' : 'B',  'USDC' : 'B+', 'USDT' : 'B+',\n",
    "                   'AAVE'  : 'B-', 'BAT'   : 'B+', 'ENJ'  : 'B+', 'ETH'  : 'A+', 'KNC'  : 'B+', 'LEND' : 'B', \n",
    "                   'LINK'  : 'B+', 'MANA'  : 'B-', 'MKR'  : 'B',  'REN'  : 'B',  'REP'  : 'B',  'SNX'  : 'C+', \n",
    "                   'UNI'   : 'B',  'WBTC'  : 'B-', 'WETH' : 'B+', 'YFI'  : 'B',  'ZRX'  : 'B+'},\n",
    "                   \n",
    "                   (\"Ethereum\", \"2020/10/27\"):\n",
    "                  {'BUSD'  : 'B',  'DAI'   : 'B',  'SUSD' : 'C+', 'TUSD' : 'B',  'USDC' : 'B+', 'USDT' : 'B+', \n",
    "                   'AAVE'  : 'C+', 'BAT'   : 'B+', 'ENJ'  : 'B+', 'ETH'  : 'A+', 'KNC'  : 'B+', 'LEND' : 'B', \n",
    "                   'LINK'  : 'B+', 'MANA'  : 'B-', 'MKR'  : 'B-', 'REN'  : 'B',  'REP'  : 'B',  'SNX'  : 'C+', \n",
    "                   'UNI'   : 'B',  'WBTC'  : 'B-', 'YFI'  : 'B',  'ZRX'  : 'B+'},\n",
    "                   \n",
    "                   (\"Ethereum\", \"2020/04/01\"):\n",
    "                  {'DAI'   : 'B-', 'USDC'  : 'B+', 'TUSD' : 'B',  'USDT' : 'B',  'SUSD' : 'C',  'BUSD' : 'B-', \n",
    "                   'SNX'   : 'C',  'REP'   : 'B-', 'ZRX'  : 'B+', 'BAT'  : 'B+', 'WBTC' : 'C',  'MKR'  : 'B-', \n",
    "                   'LINK'  : 'B+', 'KNC'   : 'B',  'MANA' : 'B-', 'LEND' : 'B',  'ETH'  : 'A',  'SETH' : 'D+'},\n",
    "                    \n",
    "                   # AmmWETH, AmmUSDT, AmmGUniDAIUSDC, AmmWBTC risk grades match most recent map above\n",
    "                   (\"AMM\", \"2021/02/24\"):\n",
    "                  {'AMMBPTBALWETH'  : 'B',  'AMMUNIUSDCWETH' : 'A-', 'AMMWETH'        : 'A',  'AMMUNIBATWETH' : 'A-',\n",
    "                   'AMMBPTWBTCWETH' : 'B+', 'AMMUSDT'        : 'B+', 'AMMUNIMKRWETH'  : 'B+', 'AMMUNICRVWETH' : 'B+',\n",
    "                   'AMMUNIUNIWETH'  : 'B+', 'AMMUNIWBTCUSDC' : 'B+', 'AMMUNIDAIUSDC'  : 'A-', 'AMMUNIAAVEWETH': 'B+',\n",
    "                   'AMMGUNIDAIUSDC' : 'A-', 'AMMUNILINKWETH' : 'A-', 'AMMUNIWBTCWETH' : 'B+', 'AMMWBTC'       : 'B',\n",
    "                   'AMMUNIYFIWETH'  : 'B+', 'AMMUNIRENWETH'  : 'B+', 'AMMUNIDAIWETH'  : 'A-', 'AMMUNISNXWETH' : 'B'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "686fd29ebc3f49988a07ba6859c6de20",
    "deepnote_cell_height": 70,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Creating Raw Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cell_id": "b30b88f1ffdc43c1a482e1817db0d117",
    "deepnote_cell_height": 117,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 1448,
    "execution_start": 1659125526536,
    "source_hash": "87a2be4d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.5 s, sys: 1.13 s, total: 14.7 s\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Collect all unique columns from queried data\n",
    "raw_db_columns = []\n",
    "\n",
    "for keys,df in query_dict.items():\n",
    "    temp_columns = list(df.columns)\n",
    "    for col in temp_columns:\n",
    "        if col in raw_db_columns:\n",
    "            continue\n",
    "        else:\n",
    "            raw_db_columns.append(col)\n",
    "\n",
    "# Create the Raw Database\n",
    "Raw_Database = pd.DataFrame(columns=raw_db_columns)\n",
    "Raw_Database.drop('borrowRateMode', axis=1, inplace=True)\n",
    "\n",
    "# Join all dataframes into Raw_Database\n",
    "for keys,df in query_dict.items():\n",
    "    if keys[2] == \"txn_data\":\n",
    "        Raw_Database = pd.concat([Raw_Database, df],ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***TEMP DOCUMENT IMPORT/EXPORT***\n",
    "test_df_matched is output of all code run above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export files to store for temporary use --> update file path \n",
    "for keys,df in query_dict.items():\n",
    "    try:\n",
    "        filename = 'PATH_NAME'+str(keys[:5])\n",
    "        df.to_parquet(filename+\".parquet\")\n",
    "    except:\n",
    "        print(keys[:5], 'not exported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Read files --> update file path \n",
    "for keys,df in query_dict.items():\n",
    "    filename = 'PATH_NAME'+str(keys[:5])+\".parquet\"\n",
    "    try:\n",
    "        query_dict[keys] = pd.read_parquet(filename)\n",
    "        print(\"Success\")\n",
    "    except:\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export file to store for temporary use --> update file path \n",
    "Raw_Database.to_parquet('PATH_NAME'+.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files --> update file path \n",
    "Raw_Database = pd.read_parquet('PATH_NAME'+.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Grade to Each Transaction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 13s, sys: 12.5 s, total: 2min 26s\n",
      "Wall time: 2min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# The minimum difference of the date at which the transaction happened and the time of rating updates will \n",
    "# determine the rating. Assumption is the closer to the rating change is a better representation of the level of \n",
    "# risk at the time of the transaction.\n",
    "\n",
    "# Stores subtset of timestamp in new row with standard ISO format\n",
    "Raw_Database['row_date'] = Raw_Database[\"timestamp\"].astype('str').str[:10]\n",
    "Raw_Database['row_date'] = pd.to_datetime(Raw_Database['row_date'])\n",
    "\n",
    "date_list = []\n",
    "\n",
    "# Calculates difference in time from each ranking of token risk\n",
    "for date in list(token_risk_dict.keys()):\n",
    "    Raw_Database[date] = abs(Raw_Database['row_date'] - datetime.strptime(date[1],\"%Y/%m/%d\"))\n",
    "\n",
    "Raw_Database['Min_Date'] = Raw_Database[[('Ethereum', '2021/09/24'),\n",
    "       ('Ethereum', '2021/08/10'), ('Ethereum', '2021/02/24'),\n",
    "       ('Ethereum', '2021/01/14'), ('Ethereum', '2020/12/01'),\n",
    "       ('Ethereum', '2020/10/27'), ('Ethereum', '2020/04/01'),\n",
    "            ('AMM', '2021/02/24')]].idxmin(axis=1)\n",
    "\n",
    "# Maps token risk with associated transaction\n",
    "Raw_Database['Min_Date'] = Raw_Database['Min_Date'].map(token_risk_dict)\n",
    "\n",
    "def get_sublist(sta,end):\n",
    "    return end[sta]\n",
    "\n",
    "Raw_Database['token_risk'] = Raw_Database.apply(lambda x: get_sublist(x['asset.symbol'], x['Min_Date']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Borrow Transaction Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.18 s, sys: 1.1 s, total: 6.29 s\n",
      "Wall time: 6.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Merges borrow rate type with the transaction type - a nuance of pulling from Messari and Aave Subgraphs\n",
    "for borrow_type in query_dict:\n",
    "    if borrow_type[2] == 'param_data':\n",
    "        Raw_Database = Raw_Database.merge(query_dict[borrow_type][['borrowRateMode','hash_unique','event_type', 'market']], \n",
    "                                          how=\"left\", on=['hash_unique','event_type', 'market'])\n",
    "        \n",
    "Raw_Database['borrowRateMode_x'].fillna(Raw_Database['borrowRateMode_y'], inplace=True)\n",
    "Raw_Database.drop('borrowRateMode_y', axis=1, inplace=True)\n",
    "Raw_Database.rename(columns={\"borrowRateMode_x\": \"borrowRateMode\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no borrows without a borrow type.\n",
      "CPU times: user 176 ms, sys: 11.6 ms, total: 188 ms\n",
      "Wall time: 194 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Total number of borrows with no borrow type value attached\n",
    "borrow_type_count = len(Raw_Database[(Raw_Database.event_type == 'borrows') & (Raw_Database['borrowRateMode'] == 'nan')]['hash_unique'].to_list())\n",
    "\n",
    "if borrow_type_count == 0:\n",
    "    print('There are no borrows without a borrow type.')\n",
    "else:\n",
    "    print('The number of borrows with no borrow type is', borrow_type_count,'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5dc7cd3472d14a25ac5b1be81a677128",
    "deepnote_cell_height": 70,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "\n",
    "## Duration Between Last Transaction Calculation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.17 s, sys: 1.63 s, total: 5.79 s\n",
      "Wall time: 6.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "days = pd.to_timedelta('00:00:00')\n",
    "Raw_Database.sort_values(['account.id','timestamp'], ascending=[False,True], inplace=True)\n",
    "Raw_Database['time_btw_txn'] = Raw_Database['timestamp'] - Raw_Database['timestamp'].shift(1)\n",
    "first_txns = Raw_Database.sort_values(['account.id','timestamp'], ascending=[False,True]).groupby('account.id').head(1)\n",
    "Raw_Database['time_btw_txn'].mask(Raw_Database['hash_unique'].isin(first_txns['hash_unique']), other=days,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Raw_Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.97 s, sys: 374 ms, total: 4.35 s\n",
      "Wall time: 4.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    # remove all columns where all values are nan/null/none\n",
    "    Raw_Database.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "except:\n",
    "    print('All columns have values.')\n",
    "    \n",
    "try:\n",
    "    Raw_Database.drop([('Ethereum', '2021/09/24'), ('Ethereum', '2021/08/10'), ('Ethereum', '2021/02/24'), \n",
    "                    ('Ethereum', '2021/01/14'), ('Ethereum', '2020/12/01'), ('Ethereum', '2020/10/27'), \n",
    "                    ('Ethereum', '2020/04/01'), ('AMM', '2021/02/24'),'Min_Date','row_date'],\n",
    "                     axis = 1, inplace=True)\n",
    "\n",
    "except:\n",
    "    print('Some columns are not in Raw_Database.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***TEMP DOCUMENT IMPORT/EXPORT***\n",
    "test_df_matched is output of all code run above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export file to store for temporary use --> update file path \n",
    "Raw_Database.to_parquet('PATH_NAME'+'.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files --> update file path \n",
    "Raw_Database = pd.read_parquet('PATH_NAME'+'.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw_Database Calculations / Derivations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 990 ms, sys: 107 ms, total: 1.1 s\n",
      "Wall time: 1.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Type of Collateral Borrow (A+ = 12, D- = 1)\n",
    "risk_int_mapping = {\"A+\" : 1.0, \"A\" : 2.0, \"A-\" : 3.0, \"B+\" : 4.0, \"B\" : 5.0, \"B-\" : 6.0,\n",
    "                    \"C+\" : 7.0, \"C\" : 8.0, \"C-\" : 9.0, \"D+\" : 10.0,\"D\" : 11.0,\"D-\" : 12.0, '0' : 0}\n",
    "# Convert risk categories to integer\n",
    "Raw_Database[\"token_risk_value\"] = Raw_Database['token_risk'].replace(risk_int_mapping)\n",
    "\n",
    "# Borrow risk value calculation\n",
    "Raw_Database['txn_risk_value'] = Raw_Database['amountUSD'] * Raw_Database['token_risk_value']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transform of AmountUSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determined from EDA. Will Require additional review as transaction volumne increase with time\n",
    "Raw_Database['txn_risk_value'] = np.log(Raw_Database['txn_risk_value']).clip(lower=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Final Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 62.9 ms, sys: 38.6 ms, total: 101 ms\n",
      "Wall time: 130 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create Final Database\n",
    "Final_Database = pd.DataFrame(Raw_Database['account.id'].drop_duplicates().copy(deep=True).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Average Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inputs:\n",
    "# event_type --> Event is found as the child entity in the query - ONLY FOR TXN_DATA \n",
    "\n",
    "def attr_derivations(event_type):\n",
    "    \n",
    "    global Final_Database\n",
    "    \n",
    "    # Loop through days for each day interval - NEED TO ADD SINCE ADDRESS INCEPTION\n",
    "    day_interval = [1,7,30,60,90,180,360,720]\n",
    "\n",
    "    # Parameters\n",
    "    #today = datetime.today()\n",
    "    today = datetime(2022,8,17, hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "    # Create attribute subset\n",
    "    df_subset = Raw_Database[Raw_Database.event_type == event_type]\n",
    "                     \n",
    "    # Sort values by account.id and then timestamp\n",
    "    df_subset.sort_values(['account.id','timestamp'], ascending=[False,True], inplace=True)\n",
    "    \n",
    "    for interval in day_interval:\n",
    "        \n",
    "        # Subtract today from each txn date: today_minus_txndate\n",
    "        df_subset_copy = df_subset.copy(deep=True)\n",
    "        df_subset_copy['today_minus' + str(interval)] = today - df_subset['timestamp']\n",
    "                     \n",
    "        # Create a subset of data that only has timestamp difference values ≤ that interval in day_interval\n",
    "        df_subset_copy = df_subset_copy[df_subset_copy['today_minus' + str(interval)] <= pd.Timedelta(str(interval)+\" days\")]\n",
    "        \n",
    "        # ROLLING TRANSACTION RISK {deposits, borrows, liquidates, withdrawals, repays}\n",
    "        df_roll_final = df_subset_copy.groupby('account.id')['txn_risk_value'].sum() / float(interval)\n",
    "        df_roll_final = df_roll_final.reset_index()\n",
    "        df_roll_final.rename(columns = {'txn_risk_value' : 'weighted_rolling_'+event_type+str(interval)+'day'}, inplace = True)\n",
    "        \n",
    "        # AVERAGE NUMBER OF EVENT TYPES {deposits, borrows, liquidates, withdrawals, repays}\n",
    "        avg_event_type_final = df_subset_copy.groupby('account.id')['event_type'].count() / float(interval)\n",
    "        avg_event_type_final = avg_event_type_final.reset_index()\n",
    "        avg_event_type_final.rename(columns = {'event_type' : 'weighted_count_'+event_type+str(interval)+'day'}, inplace = True)\n",
    "        \n",
    "        # Merge on Final_Database\n",
    "        Final_Database = Final_Database.merge(df_roll_final, on=['account.id'], how='left')\n",
    "        Final_Database = Final_Database.merge(avg_event_type_final, on=['account.id'], how='left')\n",
    "    \n",
    "    \n",
    "    # AVERAGE EVENT ACTIVITY DORMANCY\n",
    "    columns = ['weighted_activity_1day', 'weighted_activity_activity7day',\n",
    "       'weighted_activity_30day', 'weighted_activity_60day',\n",
    "       'weighted_activity_90day', 'weighted_activity_180day',\n",
    "       'weighted_activity_360day', 'weighted_activity_720day']\n",
    "    if any(column in columns for column in Final_Database.columns):\n",
    "        pass\n",
    "    else:\n",
    "        Raw_Database.sort_values(['account.id','timestamp'], ascending=[False,True], inplace=True)\n",
    "    \n",
    "        for interval in day_interval:\n",
    "            \n",
    "            # Subtract today from each txn date: today_minus_txndate\n",
    "            Raw_Database_copy = Raw_Database.copy(deep=True)\n",
    "            Raw_Database_copy['today_minus' + str(interval)] = today - df_subset['timestamp']\n",
    "            \n",
    "            # Create a subset of data that only has timestamp difference values ≤ that interval in day_interval\n",
    "            Raw_Database_copy = Raw_Database_copy[Raw_Database_copy['today_minus' + str(interval)] <= pd.Timedelta(str(interval)+\" days\")]\n",
    "            \n",
    "            # ROLLING TRANSACTION RISK {deposits, borrows, liquidates, withdrawals, repays}\n",
    "            df_roll_final = Raw_Database_copy.groupby('account.id')['txn_risk_value'].sum() / float(interval)\n",
    "            df_roll_final = df_roll_final.reset_index()\n",
    "            df_roll_final.rename(columns = {'txn_risk_value' : 'weighted_activity_'+str(interval)+'day'}, inplace = True)\n",
    "            \n",
    "            # Merge on Final_Database\n",
    "            Final_Database = Final_Database.merge(df_roll_final, on=['account.id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.7 s, sys: 4.56 s, total: 22.3 s\n",
      "Wall time: 23.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Iterating through each event type. Can be found in query_pairs\n",
    "event_type_list = [*set([item[2] for item in list(api_dict.keys())])]\n",
    "\n",
    "for event in event_type_list:\n",
    "    attr_derivations(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## % of Variable vs Stable Rate Loans\n",
    "Weighted average number of loans that were stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 795 ms, sys: 138 ms, total: 933 ms\n",
      "Wall time: 962 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Total sum of transactional risk value\n",
    "total_risk_subset = Raw_Database.groupby('account.id')['txn_risk_value'].sum()\n",
    "\n",
    "# Subset of type variable borrow transactional risk value\n",
    "var_percent = Raw_Database[Raw_Database['borrowRateMode'] == 'Variable'].groupby('account.id')['txn_risk_value'].sum() / total_risk_subset\n",
    "var_percent = var_percent.reset_index()\n",
    "var_percent.rename(columns = {'txn_risk_value' : 'var_percent'}, inplace = True)\n",
    "Final_Database = Final_Database.merge(var_percent, on=['account.id'], how='left')\n",
    "\n",
    "# Subset of type variable stable transactional risk value\n",
    "stable_percent = Raw_Database[Raw_Database['borrowRateMode'] == 'Stable'].groupby('account.id')['txn_risk_value'].sum() / total_risk_subset\n",
    "stable_percent = stable_percent.reset_index()\n",
    "stable_percent.rename(columns = {'txn_risk_value' : 'stable_percent'}, inplace = True)\n",
    "Final_Database = Final_Database.merge(stable_percent, on=['account.id'], how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## % of Transactions on Each Market\n",
    "Weighted proportion of transactions on each market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.39 s, sys: 266 ms, total: 1.65 s\n",
      "Wall time: 1.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Iterating through each event type. Can be found in query_pairs\n",
    "markets_list = [*set([item[1] for item in list(api_dict.keys())])]\n",
    "\n",
    "# Total sum of transactional risk value\n",
    "total_risk_subset = Raw_Database.groupby('account.id')['txn_risk_value'].sum()\n",
    "\n",
    "# Iterations of each market subset's transactional risk value\n",
    "for market in markets_list:\n",
    "    market_percent = Raw_Database[Raw_Database['market'] == market].groupby('account.id')['txn_risk_value'].sum() / total_risk_subset\n",
    "    market_percent = market_percent.reset_index()\n",
    "    market_percent.rename(columns = {'txn_risk_value' : str(market)+'_percent'}, inplace = True)\n",
    "    Final_Database = Final_Database.merge(market_percent, on=['account.id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***TEMP DOCUMENT IMPORT/EXPORT***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export file to store for temporary use --> update file path \n",
    "Final_Database.to_parquet('PATH_NAME'+'.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files --> update file path\n",
    "Final_Database = pd.read_parquet('PATH_NAME'+'.parquet\")"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [
   {
    "cellId": "219437313721413680b7e9b16da585c8",
    "msgId": "73af98f6-adbb-4037-bc91-852513cff965",
    "sessionId": "640d5d10-2eeb-4ed5-8718-7d1922e3fe49"
   },
   {
    "cellId": "c1f169761cd14a718b4882e4f44e9d99",
    "msgId": "3e075994-efa2-4bbb-8aec-fa4327a1d2db",
    "sessionId": "640d5d10-2eeb-4ed5-8718-7d1922e3fe49"
   }
  ],
  "deepnote_notebook_id": "cae3674f-8e84-4d3f-92ba-6193e88b6841",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
